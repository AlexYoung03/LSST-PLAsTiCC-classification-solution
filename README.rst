Solution to LSST-PLAsTiCC photometric transient classification challenge
=========================================================================

Data visualisation
=====================

Meet the classes!

* **Galactic**

  * **class_6**  | `i-timeseries <viz/target6_3timeseries-normed.png>`__ | `SEDpeak <viz/target6_SEDpeak.png>`__ | `SEDevol <viz/target6_SEDevol.png>`__ | `dipstat <viz/target6_dipstat.png>`__ | `slopecolor <viz/target6_slopecolor.png>`__ | `slopeevol <viz/target6_slopeevol.png>`__ | `tempevol <viz/target6_Tseries.png>`__ | `dist <viz/target6_dist.png>`__
  * **class_16**  | `i-timeseries <viz/target16_3timeseries-normed.png>`__ | `SEDpeak <viz/target16_SEDpeak.png>`__ | `SEDevol <viz/target16_SEDevol.png>`__ | `dipstat <viz/target16_dipstat.png>`__ | `slopecolor <viz/target16_slopecolor.png>`__ | `slopeevol <viz/target16_slopeevol.png>`__ | `tempevol <viz/target16_Tseries.png>`__ | `dist <viz/target16_dist.png>`__
  * **class_53**  | `i-timeseries <viz/target53_3timeseries-normed.png>`__ | `SEDpeak <viz/target53_SEDpeak.png>`__ | `SEDevol <viz/target53_SEDevol.png>`__ | `dipstat <viz/target53_dipstat.png>`__ | `slopecolor <viz/target53_slopecolor.png>`__ | `slopeevol <viz/target53_slopeevol.png>`__ | `tempevol <viz/target53_Tseries.png>`__ | `dist <viz/target53_dist.png>`__
  * **class_65**  | `i-timeseries <viz/target65_3timeseries-normed.png>`__ | `SEDpeak <viz/target65_SEDpeak.png>`__ | `SEDevol <viz/target65_SEDevol.png>`__ | `dipstat <viz/target65_dipstat.png>`__ | `slopecolor <viz/target65_slopecolor.png>`__ | `slopeevol <viz/target65_slopeevol.png>`__ | `tempevol <viz/target65_Tseries.png>`__ | `dist <viz/target65_dist.png>`__
  * **class_92**  | `i-timeseries <viz/target92_3timeseries-normed.png>`__ | `SEDpeak <viz/target92_SEDpeak.png>`__ | `SEDevol <viz/target92_SEDevol.png>`__ | `dipstat <viz/target92_dipstat.png>`__ | `slopecolor <viz/target92_slopecolor.png>`__ | `slopeevol <viz/target92_slopeevol.png>`__ | `tempevol <viz/target92_Tseries.png>`__ | `dist <viz/target92_dist.png>`__

* **Extragalactic**

  * **class_15**  | `i-timeseries <viz/target15_3timeseries-normed.png>`__ | `SEDpeak <viz/target15_SEDpeak.png>`__ | `SEDevol <viz/target15_SEDevol.png>`__ | `dipstat <viz/target15_dipstat.png>`__ | `slopecolor <viz/target15_slopecolor.png>`__ | `slopeevol <viz/target15_slopeevol.png>`__ | `tempevol <viz/target15_Tseries.png>`__ | `dist <viz/target15_dist.png>`__
  * **class_42**  | `i-timeseries <viz/target42_3timeseries-normed.png>`__ | `SEDpeak <viz/target42_SEDpeak.png>`__ | `SEDevol <viz/target42_SEDevol.png>`__ | `dipstat <viz/target42_dipstat.png>`__ | `slopecolor <viz/target42_slopecolor.png>`__ | `slopeevol <viz/target42_slopeevol.png>`__ | `tempevol <viz/target42_Tseries.png>`__ | `dist <viz/target42_dist.png>`__
  * **class_52**  | `i-timeseries <viz/target52_3timeseries-normed.png>`__ | `SEDpeak <viz/target52_SEDpeak.png>`__ | `SEDevol <viz/target52_SEDevol.png>`__ | `dipstat <viz/target52_dipstat.png>`__ | `slopecolor <viz/target52_slopecolor.png>`__ | `slopeevol <viz/target52_slopeevol.png>`__ | `tempevol <viz/target52_Tseries.png>`__ | `dist <viz/target52_dist.png>`__
  * **class_62**  | `i-timeseries <viz/target62_3timeseries-normed.png>`__ | `SEDpeak <viz/target62_SEDpeak.png>`__ | `SEDevol <viz/target62_SEDevol.png>`__ | `dipstat <viz/target62_dipstat.png>`__ | `slopecolor <viz/target62_slopecolor.png>`__ | `slopeevol <viz/target62_slopeevol.png>`__ | `tempevol <viz/target62_Tseries.png>`__ | `dist <viz/target62_dist.png>`__
  * **class_64**  | `i-timeseries <viz/target64_3timeseries-normed.png>`__ | `SEDpeak <viz/target64_SEDpeak.png>`__ | `SEDevol <viz/target64_SEDevol.png>`__ | `dipstat <viz/target64_dipstat.png>`__ | `slopecolor <viz/target64_slopecolor.png>`__ | `slopeevol <viz/target64_slopeevol.png>`__ | `tempevol <viz/target64_Tseries.png>`__ | `dist <viz/target64_dist.png>`__
  * **class_67**  | `i-timeseries <viz/target67_3timeseries-normed.png>`__ | `SEDpeak <viz/target67_SEDpeak.png>`__ | `SEDevol <viz/target67_SEDevol.png>`__ | `dipstat <viz/target67_dipstat.png>`__ | `slopecolor <viz/target67_slopecolor.png>`__ | `slopeevol <viz/target67_slopeevol.png>`__ | `tempevol <viz/target67_Tseries.png>`__ | `dist <viz/target67_dist.png>`__
  * **class_88**  | `i-timeseries <viz/target88_3timeseries-normed.png>`__ | `SEDpeak <viz/target88_SEDpeak.png>`__ | `SEDevol <viz/target88_SEDevol.png>`__ | `dipstat <viz/target88_dipstat.png>`__ | `slopecolor <viz/target88_slopecolor.png>`__ | `slopeevol <viz/target88_slopeevol.png>`__ | `tempevol <viz/target88_Tseries.png>`__ | `dist <viz/target88_dist.png>`__
  * **class_90**  | `i-timeseries <viz/target90_3timeseries-normed.png>`__ | `SEDpeak <viz/target90_SEDpeak.png>`__ | `SEDevol <viz/target90_SEDevol.png>`__ | `dipstat <viz/target90_dipstat.png>`__ | `slopecolor <viz/target90_slopecolor.png>`__ | `slopeevol <viz/target90_slopeevol.png>`__ | `tempevol <viz/target90_Tseries.png>`__ | `dist <viz/target90_dist.png>`__
  * **class_95**  | `i-timeseries <viz/target95_3timeseries-normed.png>`__ | `SEDpeak <viz/target95_SEDpeak.png>`__ | `SEDevol <viz/target95_SEDevol.png>`__ | `dipstat <viz/target95_dipstat.png>`__ | `slopecolor <viz/target95_slopecolor.png>`__ | `slopeevol <viz/target95_slopeevol.png>`__ | `tempevol <viz/target95_Tseries.png>`__ | `dist <viz/target95_dist.png>`__

The above visualisations were produced with extract_examples.py and show:

* i-timeseries i-band photometry, normalised to peak
* SED at peak
* SED evolution
* dipstat: Some basic statistics about number of data points, number of dips, peaks and consecutive runs (up-up-up)
* slopeevol: slope of rise and fall
* slopecolor: color evolution at rise and fall
* tempevol: evolution of black-body temperature
* dist: log(flux) distribution compared to median


Approach
----------

This data set provides some challenges:

1) Fluxes can be wrong (very negative) because of wrong subtraction -- the flux errors do not include this.
2) Heterogeneous and poor time series sampling. Many object classes show exponential light curve rises/falls. Others show periodic behaviour.
3) Different photometric bands, making the time series analysis a 2d problem.
4) Redshift outliers
5) Imbalanced object classes
6) Differences in training and test set (redshifts, DDF)

To address 1-3), I first extracted custom-made features from the time series, including simple log-linear fitting.
To address 5), I use SVC and RandomForest which support imbalanced datasets.
However I also run other methods (KNN, ANN).
Additionally I combine multiple types of classifiers with a hyper-classifier.

I like Random Forests because it requires no normalisation and still can be interpreted.

I made almost no assumptions about the transient physics, but tried to work in rest-frame when characterizing the SED.

1. Data Transform: Making time series features
------------------------------------------------

To create feature files for the training set, run::

	$ python make_std_features.py training_set
	$ python make_2dslope_features.py training_set
	$ python make_SED_features.py training_set
	$ SEDTRANSFORMER=1 python make_SED_features.py training_set

The first (make_std_features.py) computes some basic features for each photometric band:

* Shapiro_wilk statistic, Lomb-Scargle amplitude ratios and dominant frequency
* median, variance, skew, kurtosis, iqr of 
  * time
  * time differences
  * log-fluxes
  * slope between points (d log(flux) / dt)
* number of dips and peaks in the lightcurve
* length and variance of runs (how often the lightcurve consecutively goes up/down)

The second (make_2dslope_features.py) computes some custom features. (Below, fitting is done with linear regression on rest-frame information)

* for each photometric band:

  * find the peak time, and measure the rise slope (before peak) and fall slope (after peak)

* compute color at peak (flux ratios from one band to the next)
* for each night with data

  * compute u-g, u-g-r and z-Y flux ratios
  * fit a black-body spectrum to photometric data
  * store black-body temperature and flux at 400nm
  * compute how many peaks and dips the SED shape has (some have 2 peaks and a dip)

* store maximum number of peaks and dips
* store mean u-g, u-g-r, y-Y ratios, etc.
* fit flux-Temp powerlaw correlation and store slope and correlation strength
* fit time-log(wavelength)-log(flux) bilinear relation.

  * Store the SED slope and time evolution. 
  * This is done for the data before and after peak separately.

* fit time-log(wavelength)-log(flux) bilinear relation plus mixing term.

  * like above, but now if the SED evolves, this should capture it.

* compute black body temperature at peak
* compute median black body temperature of data before peak
* compute median black body temperature of data after peak

The third (make_SED_features.py) simply extracts all SEDs with their photo-z and target class.
The forth uses train_SED.py to train a simple neural network to give target class probabilities simply from the SEDs. 
Because each object can have multiple SEDs, these are combined by taking the maximum across each classification.

Before switching to make_2dslope_features.py, I created make_slope_features.py.
It performs bootstrapped least square line fits to get rise and fall slopes, peak fluxes and SED evolution over time.
This is done for each photometric band. The colors at the peaks are also computed.

Running each of these takes ~30 minutes for the 7849 training samples on one laptop CPU.

2. Data Transform of test data
--------------------------------------------

The test data are fairly large, so we parallelise here.

First we split test_set.csv into 32 chunks::

	$ bash split.sh

This will make a chunks/ folder with 32 files such as chunks/test_set_chuns1.csv, chunks/test_set_chuns2.csv, ...

Create features for all of these with parallelised make (adjust the -j argument to your number of processors):

	$ make -j4 -k chunks/test_set_chunk{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32}_{std,colorslope,SEDprob}_features.txt

Next, we want to merge all these chunks again and create test and training datasets
for galactic (photoz==0) and extragalactic sources (photoz!=0).

Conveniently, you just need:

	$ make -j {gal,exgal}/{training_set,test_set}.csv.gz

Which merges the data files together and creates the necessary links.

This is neat because I can edit/add a feature script and rerun only one 
of the analyses, without needing to rerun everything.


3. Training models
-----------------------------------------------

Now we can try to fit some models. Go into the gal/ or exgal/ folder and run::

	$ TRAINING_FILE=training_set.csv.gz python ../train_randomforest.py
	4.37 +- 0.26 RandomForest4 (training speed: 5.2s)
	3.39 +- 0.60 RandomForest10 (training speed: 0.3s)
	1.68 +- 0.58 RandomForest40 (training speed: 0.3s)
	1.22 +- 0.36 RandomForest100 (training speed: 0.7s)
	0.96 +- 0.33 RandomForest400 (training speed: 1.8s)
	1.41 +- 0.17 AdaBoost40 (training speed: 1.5s)
	1.35 +- 0.25 AdaBoost400 (training speed: 12.3s)
	1.84 +- 0.51 ExtraTrees40 (training speed: 0.3s)

This will train a few types of randomforests and tell you their statistics (logloss, less is better). Here we see that a Random Forest with 400 features does best among these methods.

If you run::

	$ TRAINING_FILE=training_set.csv.gz PREDICT_FILE=test_set.csv.gz python ../train_randomforest.py

It will store the class prediction (already in the format expected for the submission).

Random Forests are nice because they can deal with uninformative features.
For other methods we first have to reduce the feature space, keeping only the most useful features.

We do this like so::

	$ export TRAINING_FILE=training_set.csv.gz 

	$ FIND_FEATURE_SUBSET=1 python ../train_randomforest.py
	$ sort -k2,2 important_columns* > important_columns.txt
	$ for i in {all,u,g,r,i,z,Y}_n{,good}measurements; do echo $i; done > blacklist_features.txt

	$ export PREDICT_FILE=test_set.csv.gz 
	$ SIMPLIFY=1 TRANSFORM=MM python ../train_knn.py
	$ SIMPLIFY=1 TRANSFORM=QTN python ../train_knn.py

This first command (``FIND_FEATURE_SUBSET=1 python ../train_randomforest.py``) runs a RandomForest and SVC classifier which each identify the top most useful features (stored in important_columns.txt)
We can then blacklist some features that we think are distracting, by adding them to blacklist_features.txt (for example, the number of measurements). 

train_knn.py runs:

* if SIMPLIFY=1, then only the reduced column set is used
* if TRANSFORM=MM,QTN,QTU, a min-max, quantile-to-normal or quantile-to-uniform transformation is applied
* PCA whitening with 10, 40 and all components
* each of

  * LinearDiscriminantAnalysis
  * SVC (support vector machine classifier)
  * KNeighborsClassifier with K=2, 4, 10, 40, 100
  * MLPClassifier neural networks (1 to few layers)

Both train_knn.py and train_randomforest.py report the quality of the prediction (lower loss is better) from K-fold cross-validation.

4. Training a meta classifier
-------------------------------------------------

Maybe one classifier does better in some parameter space region, but another does better in another parameter space region.
So we want to combine the classifiers.
Basically we just use their probability output as an input to another machine learning method.
Here is a sketch::

	┌────────────────────────┐
	│    Random Forest       │ ───┐
	└────────────────────────┘    │
	                              │
	┌────────────────────────┐    │
	│ Support Vector Classif │ ───┤
	└────────────────────────┘    │       ┌───────────────────┐
	                              ├─────> │  MLP              │ ─> submit
	┌────────────────────────┐    │       └───────────────────┘
	│         MLP            │ ───┤
	└────────────────────────┘    │
	                              │
	┌────────────────────────┐    │
	│ Linear Discriminant A. │ ───┘
	└────────────────────────┘


For example::
	
	# for galactic:
	$ METHOD=MLP python ../hyperpredictor.py RandomForest400 SIMPLEQTN-PCA40-SVC-default SIMPLEQTN-PCA40-MLP4 SIMPLEMM-PCA40-LDA
	loading training_set.csv.gz_predictions_RandomForest400.csv.gz ...
	loading test_set.csv.gz_predictions_RandomForest400.csv.gz ...
	loading training_set.csv.gz_predictions_SIMPLEQTN-PCA40-SVC-default.csv.gz ...
	loading test_set.csv.gz_predictions_SIMPLEQTN-PCA40-SVC-default.csv.gz ...
	loading training_set.csv.gz_predictions_SIMPLEQTN-PCA40-MLP4.csv.gz ...
	loading test_set.csv.gz_predictions_SIMPLEQTN-PCA40-MLP4.csv.gz ...
	loading training_set.csv.gz_predictions_SIMPLEMM-PCA40-LDA.csv.gz ...
	loading test_set.csv.gz_predictions_SIMPLEMM-PCA40-LDA.csv.gz ...
	0.771 +- 0.121  # <--- this is the log loss
	training done after 59.6s
	Confusion matrix:
	       6   16   53   65   92
	  6  113   20    4   14    0
	 16    0  849    0   60   15
	 53    4    1   21    4    0
	 65    5   72    0  894   10
	 92    0   23    1    2  213
	Confusion matrix, normalised:
	       6   16   53   65   92
	  6   74   13    2    9    0
	 16    0   91    0    6    1
	 53   13    3   70   13    0
	 65    0    7    0   91    1
	 92    0    9    0    0   89
	Confusion examples:
	6 confused as 16: 282647,9683805,10343540,12022536
	53 confused as 6: 133773,278480,4416529,17991828
	53 confused as 65: 4855013,33834663,106195942,125642419
	Predicting ...
	  predictions for training data...
	    saving ...
	  predictions for unknown data...
	    saving ...
	predictions done after 155.5s
	
	# for extragalactic
	$ METHOD=MLP python ../hyperpredictor.py RandomForest400 SIMPLEQTN-PCA40-SVC-default SIMPLEQTN-PCA40-MLP4 SIMPLEMM-PCA40-LDA
	loading training_set.csv.gz_predictions_RandomForest400.csv.gz ...
	loading test_set.csv.gz_predictions_RandomForest400.csv.gz ...
	loading training_set.csv.gz_predictions_SIMPLEQTN-PCA40-SVC-default.csv.gz ...
	loading test_set.csv.gz_predictions_SIMPLEQTN-PCA40-SVC-default.csv.gz ...
	loading training_set.csv.gz_predictions_SIMPLEQTN-PCA40-MLP4.csv.gz ...
	loading test_set.csv.gz_predictions_SIMPLEQTN-PCA40-MLP4.csv.gz ...
	loading training_set.csv.gz_predictions_SIMPLEMM-PCA40-LDA.csv.gz ...
	loading test_set.csv.gz_predictions_SIMPLEMM-PCA40-LDA.csv.gz ...
	training MLP...
	1.342 +- 0.202  # <--- this is the log loss
	training done after 99.5s
	Confusion matrix:
	      15   42   52   62   64   67   88   90   95
	 15  320   44    0   15    2    6    9   97    2
	 42   56  543   73   86   16   11   14  386    8
	 52    3   54   16   13    1    2    0   94    0
	 62    7  108   26  153   17   31    0  142    0
	 64    0    6    1    5   86    0    2    2    0
	 67    2   15    4   30    3   53    0  101    0
	 88   10    6    0    0    1    0  324   15   14
	 90   60  123   17   38    4   19    7  2041    4
	 95    1    6    2    0    0    0   17    9  140
	Confusion matrix, normalised:
	      15   42   52   62   64   67   88   90   95
	 15   64    8    0    3    0    1    1   19    0
	 42    4   45    6    7    1    0    1   32    0
	 52    1   29    8    7    0    1    0   51    0
	 62    1   22    5   31    3    6    0   29    0
	 64    0    5    0    4   84    0    1    1    0
	 67    0    7    1   14    1   25    0   48    0
	 88    2    1    0    0    0    0   87    4    3
	 90    2    5    0    1    0    0    0   88    0
	 95    0    3    1    0    0    0    9    5   80
	Confusion examples:
	15 confused as 90: 97406,113669,133234,148996
	42 confused as 90: 1632,2103,2300,3285
	52 confused as 42: 64248,157746,209796,211331
	52 confused as 90: 10757,11773,13138,14279
	62 confused as 42: 62908,81252,140948,197559
	62 confused as 90: 18645,26338,39846,49529
	67 confused as 62: 3041,233697,276457,283066
	67 confused as 90: 34437,60554,72385,77518
	Predicting ...
	  predictions for training data...
	    saving ...
	  predictions for unknown data...
	    saving ...
	predictions done after 423.1s


The hyperpredictor (meta classifier) often has a substantially better quality than any individual classifiers. 
For example, in gal the 0.771 is better than 0.96 from RandomForest400 alone.


5. Novelty detection
-------------------------

Here I tried only some simple approaches, and this can definitely be improved.

This runs Isolation forest and Ellipsoid novelty detections with various false positive thresholds::

	$ export TRAINING_FILE=training_set.csv.gz
	$ export PREDICT_FILE=test_set.csv.gz
	$ TRANSFORM=QTN python ../train_novel.py
	...
	$ TRANSFORM=MM python ../train_novel.py
	reading data file to predict ...
	unknown: (390510, 427)
	running MM-EllEnvelope-0.4: training speed: 403.2s
	predictions for unknown data...
	novel: 390510/390510 (100.00%)
	predictions done after 39.6s
	running MM-IsolForest-0.4: training speed: 1.2s
	predictions for unknown data...
	novel: 241614/390510 (61.87%)
	predictions done after 128.8s
	running MM-EllEnvelope-0.1: training speed: 243.2s
	predictions for unknown data...
	novel: 278/390510 (0.07%)
	predictions done after 37.5s
	running MM-IsolForest-0.1: training speed: 1.2s
	predictions for unknown data...
	

I also tried an autoencoder in train_novel_autoencoder.py

Experimental methods
-------------------------

I tried to kmeans cluster the test and training data, and give each cluster
classification probabilities proportional to what test data it contains.

So if a cluster has 3 class92 objects and 16 class62 objects, it would get 3/20 probability for class92, 16/20 probability for class62 and 1/20 probability for class99.
I just pretend each cluster has a class99 member. This has the nice property that 
the fewer training set objects are in the cluster, the more likely the cluster is novel.

All parameters are configurable::

	$ export TRAINING_FILE=training_set.csv.gz
	$ export PREDICT_FILE=test_set.csv.gz
	$ K=20 SIMPLIFY=1 NPCACOMP=30 PROB_FLATNESS=1 FLATPRIOR_STRENGTH=0.1 OUTLIERS_STRENGTH=1.0 TRANSFORM=MM TRAINING_FILE=training_set.csv.gz python ../train_kmeans.py
	running SIMPLEPCA30-Kmeans20 ...
	PCA dimensionality reduction done after 8.159s
	PCA Variance ratios: [0.6443876  0.08210639 0.0365373  0.0341573  0.02762621 0.0249804
	 0.02060876 0.01441821 0.01133848 0.01060707 0.00929893 0.00772659
	 0.00745863 0.00704904 0.00661704 0.00575273 0.00517016 0.00496061
	 0.00457319 0.00401399 0.00387652 0.00324556 0.00291135 0.00274897
	 0.00262517 0.00246217 0.00226804 0.00185096 0.00159308 0.0012537 ]
	clustering done after 2.866s
	cluster  0:   0/ 34455 |  6  0  6  0  0  6  0  0  6  0  0  0  6  0 66 ***
	cluster  1: 258/ 28855 |  1  0 50  0  0  0  0  0 47  0  0  0  0  0  0 
	cluster  2: 400/ 46322 |  4  0 69  0  0  1  0  0 24  0  0  0  0  0  0 
	cluster  3:   6/ 11214 |  1  0  1  0  0 14  0  0 68  0  0  0  1  0 13 
	cluster  4: 163/ 19624 |  1  0 41  0  0  0  0  0 55  0  0  0  0  0  0 
	cluster  5:   2/  1484 |  2  0 31  0  0  2  0  0 31  0  0  0  2  0 28 
	cluster  6:   0/ 29861 |  6  0  6  0  0  6  0  0  6  0  0  0  6  0 66 ***
	cluster  7:  57/  6545 |  0  0 27  0  0  1  0  0 66  0  0  0  1  0  1 
	cluster  8: 148/ 15968 | 10  0  3  0  0  0  0  0 83  0  0  0  0  0  0 
	cluster  9: 612/     0 |  7  0 49  0  0  3  0  0  1  0  0  0 37  0  0 
	cluster 10:  29/  5297 |  0  0  3  0  0  0  0  0 88  0  0  0  3  0  3 
	cluster 11:  71/ 24022 |  0  0 52  0  0  0  0  0 44  0  0  0  1  0  1 
	cluster 12: 204/ 20576 |  2  0 16  0  0  0  0  0 80  0  0  0  0  0  0 
	cluster 13:   7/ 19428 | 24  0 48  0  0  1  0  0 12  0  0  0  1  0 11 
	cluster 14: 149/ 10241 | 30  0  0  0  0  2  0  0 66  0  0  0  0  0  0 
	cluster 15:   0/ 24909 |  6  0  6  0  0  6  0  0  6  0  0  0  6  0 66 ***
	cluster 16: 204/ 24189 |  2  0 19  0  0  0  0  0 76  0  0  0  0  0  0 
	cluster 17:   5/ 32894 | 63  0  1  0  0  1  0  0 16  0  0  0  1  0 15 
	cluster 18:   8/ 30502 |  1  0  1  0  0  1  0  0 85  0  0  0  1  0 10 
	cluster 19:   2/  4124 | 31  0 31  0  0  2  0  0  2  0  0  0  2  0 28 
	storing under 'test_set.csv.gz_predictions_SIMPLEPCA30-Kmeans20_flatness1.0_prior0.1_outliers1.0.csv.gz' ...

This uses 20 clusters, after simplifying the parameter columns and applying a 30-component PCA.
Besides the votes from the training set members giving the fractions,
each cluster is additionally  given a outlier vote of 1.0 and a 0.1 vote for each class.

The clusters highlighted with `***` have no training set members.
"0/ 34455" indicates number of training set / test set members. The other columns show the computed fractions in per cent for each class.


6. Blending submissions
--------------------------

Now we want to combine novelty detection or fixed outlier fractions with our
hyperpredictor (or another method).::

	┌─────────────────────────────────┐
	│       Classifying Method        │ 
	│       (e.g. RandomForest)       │ ───┐
	└─────────────────────────────────┘    │    ┌─────────────┐
	                                       ├──> │    Blend    │ ─> submit
	┌─────────────────────────────────┐    │    └─────────────┘
	│  Novelty Detection Method       │ ───┘
	│      (e.g. IsolForest)          │ 
	└─────────────────────────────────┘    


At this point you should have many prediction files, such as::

	test_set.csv.gz_hyperpredictions-MLP.csv.gz
	test_set.csv.gz_predictions_AdaBoost400.csv.gz
	test_set.csv.gz_predictions_AdaBoost40.csv.gz
	test_set.csv.gz_predictions_ExtraTrees40.csv.gz
	test_set.csv.gz_predictions_RandomForest100.csv.gz
	test_set.csv.gz_predictions_RandomForest10.csv.gz
	test_set.csv.gz_predictions_RandomForest400.csv.gz
	test_set.csv.gz_predictions_RandomForest40.csv.gz
	test_set.csv.gz_predictions_RandomForest4.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-KNN100.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-KNN10.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-KNN2.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-KNN40.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-KNN4.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-LDA.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-MLP10-20-10.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-MLP10.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-MLP40.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-MLP4-16-4.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-MLP4.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-SVC-0.1.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA10-SVC-default.csv.gz
	test_set.csv.gz_predictions_SIMPLEMM-PCA40-KNN100.csv.gz
	...
	test_set.csv.gz_predictions_SIMPLEMM-PCA56-SVC-default.csv.gz
	test_set.csv.gz_predictions_SIMPLEQTN-PCA10-KNN100.csv.gz
	...
	test_set.csv.gz_predictions_SIMPLEQTN-PCA56-SVC-default.csv.gz

Lets choose one of them, test_set.csv.gz_hyperpredictions-MLP.csv.gz
 (you can choose RandomForest400 for simplicity).

First we need to merge the gal/ and exgal/ predictions.

Going into the main folder, we run::

	$ bash merge.sh test_set.csv.gz_hyperpredictions-MLP.csv.gz

this gives us a full test_set.csv.gz_hyperpredictions-MLP.csv.gz file in the right format.

You also want to merge the detected outliers, for example::

	$ cat {exgal,gal}/test_set.csv.gz_novel_SIMPLEMM-IsolForest-0.1.csv > test_set.csv.gz_novel_SIMPLEMM-IsolForest-0.1.csv

Now we only have to:

* add novelty detection (class_99 probabilities)
* use the classifier predictions
* modify the prodictions by adding a prior, flattening them with an exponent

To start, lets add a constant outlier probability of 10% to all objects, and add to each class a 1% probability, i.e. in pseudo-code::

	class_i = class_i + 0.01 
	class_99 = 0.1
	normalise_classes()

::

	$ PRIOR_STRENGTH=0.01 PRIOR_STRENGTH_OUTLIERS=0.1 EXPO=1 python blend_outliers.py test_set.csv.gz_hyperpredictions-MLP.csv.gz
	loading prediction to correct, "test_set.csv.gz_hyperpredictions-MLP.csv.gz" ...
	  adjusting column "class_6" ...
	  adjusting column "class_15" ...
	  adjusting column "class_16" ...
	  adjusting column "class_42" ...
	  adjusting column "class_52" ...
	  adjusting column "class_53" ...
	  adjusting column "class_62" ...
	  adjusting column "class_64" ...
	  adjusting column "class_65" ...
	  adjusting column "class_67" ...
	  adjusting column "class_88" ...
	  adjusting column "class_90" ...
	  adjusting column "class_92" ...
	  adjusting column "class_95" ...
	  adjusting column "class_99" ...
	normalising columns ...
	writing data to "test_set.csv.gz_hyperpredictions-MLP.csv.gz_blend_expo1.0prior0.01_outlierprior0.1.csv.gz"...


Setting the exponent EXPO to 0.5 takes the square root of the classifier probabilities. This flattens the predictions, making them less sharp::

	class_i = class_i**EXPO + PRIOR_STRENGTH

But we want to insert novelty detections as class_99 results. For example, to give all those detected with the Isolation forest (at 0.1 false positive fraction) a outlier probability as strong as all the other classes combined (OUTLIER_CONF=1.0) and giving others a 1% class_99 probability. i.e. in pseudo-code::

	class_i = class_i + 0.01 
	class_99 = 0.01 + (1.0 if IsolForest-detected else 0)
	normalise_classes()

To do this, we run::

	$ PRIOR_STRENGTH=0.01 PRIOR_STRENGTH_OUTLIERS=0.01 EXPO=1 OUTLIER_CONF=1.0 OUTLIER_METHOD=SIMPLEMM-IsolForest-0.1 python blend_outliers.py test_set.csv.gz_hyperpredictions-MLP.csv.gz
	loading prediction to correct, "test_set.csv.gz_hyperpredictions-MLP.csv.gz" ...
	loading outlier votes of SIMPLEMM-IsolForest-0.1 ...
	5.00% outliers
	  adjusting column "class_6" ...
	  adjusting column "class_15" ...
	  adjusting column "class_16" ...
	  adjusting column "class_42" ...
	  adjusting column "class_52" ...
	  adjusting column "class_53" ...
	  adjusting column "class_62" ...
	  adjusting column "class_64" ...
	  adjusting column "class_65" ...
	  adjusting column "class_67" ...
	  adjusting column "class_88" ...
	  adjusting column "class_90" ...
	  adjusting column "class_92" ...
	  adjusting column "class_95" ...
	  adjusting column "class_99" ...
	normalising columns ...
	writing data to "test_set.csv.gz_hyperpredictions-MLP.csv.gz_blend_expo1.0prior0.01_SIMPLEMM-IsolForest-0.1outlierconf1.0prior0.01.csv.gz"...


7. Submitting to Kaggle
-----------------------------

Finally, we are ready to submit a result file::

	$ kaggle competitions submit -c PLAsTiCC-2018 -f test_set.csv.gz_hyperpredictions-MLP.csv.gz_blend_expo1.0prior0.01_SIMPLEMM-IsolForest-0.1outlierconf1.0prior0.01.csv.gz -m 'my submission'
	$ # (the above needs:) 
	$ # pip install kaggle --user

And look at the score::

	$ kaggle competitions submissions -c PLAsTiCC-2018|head -n3


Open issues and Things that did not work
-----------------------------------------

The best score I received on kaggle is 1.790. I am stuck at this barrier.
I think there are some issues remaining, because 

* Classifiers give *much, much better* scores on the training set than upon submission.
* I get the best results when I apply flattening exponents of 0.1 and add high outlier and flat class probabilities (4%). This again means the classifiers are not as useful as they seem.

This indicates that although the classifiers seem to work fine locally, they do not do well on the Kaggle evaluation.

This could be

* Issue with submission format -- but it looks fine to me. I added example files to the repository (test_set.csv.gz_predictions_RandomForest400_woutliers.csv.gz training_set.csv.gz_predictions_RandomForest400.csv.gz).
* Issue with my logloss scoring function -- but the random forest is trained independent of the scoring function
* Issue with novelty detection (class_99). Definitely worth improving, submitting various approaches did not move the needle though. Looking at the detected cases, I did not find any obvious candidates for novel classes.
* Issue with exgal/gal misclassifications. Maybe some galactic sources are in fact extragalactic?
* Issue with redshifts. I applied some redshift resampling techniques (see resample_training.py), but it had no good impact.
* Issues with classifier accuracy. Most confusion remaining is

  * "x->y" means x is mistaken for y
  * 6->16 (e.g. object_id=282647,9683805,10343540,12022536)
  * 53->6 (133773,278480,4416529,17991828) 
  * 53->65 (4855013,33834663,106195942,125642419)
  * 15,42,52,62,67->90 (97406,113669,133234,148996, 1632,2103,2300,3285, 10757,13138,14279,15718, 18645,19213,26338,39846, 28636,34437,60554,72385)
  * 52,62,67->42 (64248,157746,213374,223905, 62908,140948,197559,334014, 45349,283066,290676,20909188)
  * 67->62 (3041,233697,276457,278959)

* ...

Any feedback is appreciated.


License
--------------
AGPLv3



